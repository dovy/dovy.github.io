```python
# Imports
import pyspark
from pyspark.sql import SQLContext
from pyspark.sql import functions as F
from pyspark.sql import functions as f
from pyspark.sql.functions import col
from pyspark.sql.types import *
from pyspark.sql.functions import *
from pyspark.context import SparkContext
import json
from pyspark.sql.functions import to_json, struct
from pyspark.sql import *
spark = SparkSession.builder.getOrCreate()
global sc
sc = SparkContext.getOrCreate()
sqlContext = SQLContext(sc)
spark = SparkSession(sc)

def clean_function(raw_json, filename):
    """
        Utility used to parse out the key => value pairs without the pain of DF manipulation.
    """
    import json
    def parse_dict(init, lkey='', sep="_"):
        # Iterates over a json dictionary, 
        ret = {}
        for rkey, val in init.items():
            # Add a replace for . notation, ensuring no issues when running queries, and remove the spaces too
            key = (lkey + rkey.replace('.', sep)).lower().replace(' ', sep)
            print(key)
            if isinstance(val, dict):
                ret.update(parse_dict(val, key + sep))
            else:
                ret[key] = val
        return ret
    
    payload = json.loads(raw_json)
    if "events" in payload and payload['events']:
        payload['events'] = payload['events'][0]['data']
    payload = parse_dict(init=payload, sep='_')
    # Now jsonify all remaining lists and dicts
    for key in payload.keys():
        value = payload[key]
        if isinstance(value, list) or isinstance(value, dict):
            if value:
                value = json.dumps(value)
            else:
                value = None
        payload[key] = value
    # Append the raw_json to the DF
    payload["raw_json"] = raw_json
    # Append the source filename for this row
    payload['source_filename'] = filename

    return json.dumps(payload)  # Return a string, not an object

# Read the source file(s)
df = sqlContext.read.text("/Users/natalie/Downloads/SoulCycle_20180629232301710993_13043.txt")

# Append the Source Filename to the dataframe, allowing us to store where things came from.
df = df.withColumn("source_filename", input_file_name())
# Strip the filename, to only have the filename, not the full path. This helps identify the source of the file.
split_col = pyspark.sql.functions.split(df['source_filename'], 'SoulCycle_')
df = df.withColumn('source_filename', split_col.getItem(1))

clean_function_udf = udf(clean_function, StringType())
df2 = df.withColumn("value", clean_function_udf(col('value'), col('source_filename')))


# df2_new.show(1, truncate=False)
df3 = sqlContext.read.json(df2.rdd.map(lambda r: r.value))
# df3.printSchema()
# df3.show()


```
